{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-10T06:58:37.040756Z","iopub.status.busy":"2023-04-10T06:58:37.039506Z","iopub.status.idle":"2023-04-10T06:58:37.046503Z","shell.execute_reply":"2023-04-10T06:58:37.045356Z","shell.execute_reply.started":"2023-04-10T06:58:37.040714Z"},"trusted":true},"outputs":[],"source":["ratio_ViT_B_16          = 0.74880\n","ratio_CLIP_Interrogator = 0.21120\n","ratio_OFA               = 0.04000"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-10T06:58:37.049868Z","iopub.status.busy":"2023-04-10T06:58:37.048734Z"},"trusted":true},"outputs":[],"source":["# Using the pre compiled wheel since we don't have internet on submission\n","!pip install -q /kaggle/input/stable-diffusion-data/transformers-4.18.0.dev0-py3-none-any.whl"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import sys\n","import glob\n","from pathlib import Path\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from transformers import OFATokenizer, OFAModel\n","from transformers.models.ofa.generate import sequence_generator\n","\n","import gc"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["CKPT_DIR = \"/kaggle/input/stable-diffusion-data/OFA-large-caption/\"\n","IMAGE_DIR = \"/kaggle/input/stable-diffusion-image-to-prompts/images\"\n","\n","BATCH_SIZE = 24"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n","resolution = 480\n","patch_resize_transform = transforms.Compose([\n","        lambda image: image.convert(\"RGB\"),\n","        transforms.Resize((resolution, resolution), interpolation=Image.BICUBIC),\n","        transforms.ToTensor(), \n","        transforms.Normalize(mean=mean, std=std)\n","    ])\n","\n","tokenizer = OFATokenizer.from_pretrained(CKPT_DIR)\n","model = OFAModel.from_pretrained(CKPT_DIR, use_cache=False).cuda()\n","txt = \" what does the image describe?\"\n","inputs = tokenizer([txt], return_tensors=\"pt\").input_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample_images = glob.glob(\"/kaggle/input/stable-diffusion-image-to-prompts/images/*\")[:7]\n","fig, ax = plt.subplots(7,1, figsize=(4,35))\n","\n","for i,impath in enumerate(sample_images):\n","    image = Image.open(impath)\n","    image_t = patch_resize_transform(image).cuda().unsqueeze(0)\n","    out = model.generate(inputs.cuda(), patch_images=image_t.cuda(), num_beams=5, no_repeat_ngram_size=2)\n","    out_captions = tokenizer.batch_decode(out, skip_special_tokens=True)\n","    ax[i].imshow(image)\n","    ax[i].text(1.1, .5, out_captions[0], horizontalalignment='left', verticalalignment='center', transform=ax[i].transAxes)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sys.path.append('../input/sentence-transformers-222/sentence-transformers')\n","from sentence_transformers import SentenceTransformer, models\n","\n","comp_path = Path('../input/stable-diffusion-image-to-prompts/')\n","st_model = SentenceTransformer('/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ImageGen(Dataset):\n","    def __init__(self, root, batch_size=32):\n","        self.root = root\n","        self.im_paths = os.listdir(self.root)\n","        self.batch_size = batch_size\n","        self.sz = len(self.im_paths)\n","        self.genlen = self.sz//self.batch_size + int(self.sz%self.batch_size > 0)\n","        \n","    def __getitem__(self, index):\n","        if index >= self.genlen:\n","            raise IndexError(\"Out of bounds\")\n","        \n","        l, r = index*self.batch_size, min(self.sz, (index+1)*self.batch_size)\n","        \n","        f_paths = [os.path.join(self.root, self.im_paths[i]) for i in range(l,r)]\n","        f_ids = [self.im_paths[i][:-4] for i in range(l,r)]\n","        \n","        ims = [Image.open(f_path) for f_path in f_paths]\n","        ims = [patch_resize_transform(im).cuda().unsqueeze(0) for im in ims]\n","        ims = torch.cat(ims)\n","        \n","        return ims, f_ids\n","    \n","    def __len__(self):\n","        return self.genlen"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sub_ids = []\n","sub_embeds = []\n","\n","imgen = ImageGen(IMAGE_DIR, BATCH_SIZE)\n","\n","for b in imgen:\n","    for j in range(len(b[1])):\n","        sub_ids.extend([f\"{b[1][j]}_{i}\" for i in range(384)])\n","    \n","    img_batch = b[0]\n","    out = model.generate(inputs.repeat(len(img_batch), 1).cuda(), patch_images=img_batch, num_beams=5, no_repeat_ngram_size=2)\n","    out_captions = tokenizer.batch_decode(out, skip_special_tokens=True)\n","    out_captions = [cap + \", fine details, masterpiece\" for cap in out_captions]\n","    \n","    embeddings = st_model.encode(out_captions).flatten()\n","    sub_embeds.extend(embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["embeddings1 = np.array(sub_embeds)\n","embeddings1.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["del model, tokenizer, st_model\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["wheels_path = \"/kaggle/input/clip-interrogator-wheels-x\"\n","clip_interrogator_whl_path = f\"{wheels_path}/clip_interrogator-0.4.3-py3-none-any.whl\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["!pip install --no-index --find-links $wheels_path $clip_interrogator_whl_path -q"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip list | grep transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["import inspect\n","import importlib\n","\n","from blip.models import blip\n","from clip_interrogator import clip_interrogator"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["# replace tokenizer path to prevent downloading\n","blip_path = inspect.getfile(blip)\n","\n","fin = open(blip_path, \"rt\")\n","data = fin.read()\n","data = data.replace(\n","    \"BertTokenizer.from_pretrained('bert-base-uncased')\", \n","    \"BertTokenizer.from_pretrained('/kaggle/input/clip-interrogator-models-x/bert-base-uncased')\"\n",")\n","fin.close()\n","\n","fin = open(blip_path, \"wt\")\n","fin.write(data)\n","fin.close()\n","\n","# reload module\n","importlib.reload(blip)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["# fix clip_interrogator bug\n","clip_interrogator_path = inspect.getfile(clip_interrogator.Interrogator)\n","\n","fin = open(clip_interrogator_path, \"rt\")\n","data = fin.read()\n","data = data.replace(\n","    'open_clip.get_tokenizer(clip_model_name)', \n","    'open_clip.get_tokenizer(config.clip_model_name.split(\"/\", 2)[0])'\n",")\n","fin.close()\n","\n","fin = open(clip_interrogator_path, \"wt\")\n","fin.write(data)\n","fin.close()\n","\n","# reload module\n","importlib.reload(clip_interrogator)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import sys\n","from PIL import Image\n","from pathlib import Path\n","import matplotlib.pyplot as plt \n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import open_clip\n","\n","\n","sys.path.append('../input/sentence-transformers-222/sentence-transformers')\n","from sentence_transformers import SentenceTransformer, models\n","\n","comp_path = Path('/kaggle/input/stable-diffusion-image-to-prompts/')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class CFG:\n","    device = \"cuda\"\n","    seed = 42\n","    embedding_length = 384\n","    sentence_model_path = \"/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2\"\n","    blip_model_path = \"/kaggle/input/clip-interrogator-models-x/model_large_caption.pth\"\n","    ci_clip_model_name = \"ViT-H-14/laion2b_s32b_b79k\"\n","    clip_model_name = \"ViT-H-14\"\n","    clip_model_path = \"/kaggle/input/clip-interrogator-models-x/CLIP-ViT-H-14-laion2B-s32B-b79K/open_clip_pytorch_model.bin\"\n","    cache_path = \"/kaggle/input/clip-interrogator-models-x\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_submission = pd.read_csv(comp_path / 'sample_submission.csv', index_col='imgId_eId')\n","df_submission.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["images = os.listdir(comp_path / 'images')\n","imgIds = [i.split('.')[0] for i in images]\n","\n","eIds = list(range(CFG.embedding_length))\n","\n","imgId_eId = [\n","    '_'.join(map(str, i)) for i in zip(\n","        np.repeat(imgIds, CFG.embedding_length),\n","        np.tile(range(CFG.embedding_length), len(imgIds))\n","    )\n","]\n","\n","assert sorted(imgId_eId) == sorted(df_submission.index)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["st_model = SentenceTransformer(CFG.sentence_model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_config = clip_interrogator.Config(clip_model_name=CFG.ci_clip_model_name)\n","model_config.cache_path = CFG.cache_path"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["configs_path = os.path.join(os.path.dirname(os.path.dirname(blip_path)), 'configs')\n","med_config = os.path.join(configs_path, 'med_config.json')\n","blip_model = blip.blip_decoder(\n","    pretrained=CFG.blip_model_path,\n","    image_size=model_config.blip_image_eval_size, \n","    vit=model_config.blip_model_type, \n","    med_config=med_config\n",")\n","blip_model.eval()\n","blip_model = blip_model.to(model_config.device)\n","model_config.blip_model = blip_model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["clip_model = open_clip.create_model(CFG.clip_model_name, precision='fp16' if model_config.device == 'cuda' else 'fp32')\n","open_clip.load_checkpoint(clip_model, CFG.clip_model_path)\n","clip_model.to(model_config.device).eval()\n","model_config.clip_model = clip_model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["clip_preprocess = open_clip.image_transform(\n","    clip_model.visual.image_size,\n","    is_train = False,\n","    mean = getattr(clip_model.visual, 'image_mean', None),\n","    std = getattr(clip_model.visual, 'image_std', None),\n",")\n","model_config.clip_preprocess = clip_preprocess"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ci = clip_interrogator.Interrogator(model_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cos = torch.nn.CosineSimilarity(dim=1)\n","\n","mediums_features_array = torch.stack([torch.from_numpy(t) for t in ci.mediums.embeds]).to(ci.device)\n","movements_features_array = torch.stack([torch.from_numpy(t) for t in ci.movements.embeds]).to(ci.device)\n","flavors_features_array = torch.stack([torch.from_numpy(t) for t in ci.flavors.embeds]).to(ci.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def interrogate(image: Image) -> str:\n","    caption = ci.generate_caption(image)\n","    image_features = ci.image_to_features(image)\n","    \n","    medium = [ci.mediums.labels[i] for i in cos(image_features, mediums_features_array).topk(1).indices][0]\n","    movement = [ci.movements.labels[i] for i in cos(image_features, movements_features_array).topk(1).indices][0]\n","    flaves = \", \".join([ci.flavors.labels[i] for i in cos(image_features, flavors_features_array).topk(3).indices])\n","\n","    if caption.startswith(medium):\n","        prompt = f\"{caption}, {movement}, {flaves}\"\n","    else:\n","        prompt = f\"{caption}, {medium}, {movement}, {flaves}\"\n","\n","    return clip_interrogator._truncate_to_fit(prompt, ci.tokenize)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["prompts = []\n","\n","images_path = \"../input/stable-diffusion-image-to-prompts/images/\"\n","for image_name in images:\n","    img = Image.open(images_path + image_name).convert(\"RGB\")\n","\n","    generated = interrogate(img)\n","    \n","    prompts.append(generated)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def add_text_limiters(text: str) -> str:\n","    return \" \".join([\n","        word + \"\\n\" if i % 15 == 0 else word \n","        for i, word in enumerate(text.split(\" \"), start=1)\n","    ])\n","\n","def plot_image(image: np.ndarray, original_prompt: str, generated_prompt: str) -> None:\n","    plt.figure(figsize=(10, 10))\n","    plt.imshow(image)\n","    plt.annotate(\n","        \"Original prompt:\\n\" + add_text_limiters(original_prompt) + \"\\n\\nGenerated prompt:\\n\" + add_text_limiters(generated_prompt), \n","        xy=(1.05, 0.5), xycoords='axes fraction', ha='left', va='center', \n","        fontsize=16, rotation=0, color=\"#104a6e\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["embeddings2 = st_model.encode(prompts).flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["del ci\n","del blip_model, clip_model\n","del st_model\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["embeddings12 = ratio_OFA * embeddings1 + ratio_CLIP_Interrogator * embeddings2"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["del embeddings1\n","del embeddings2\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import timm\n","from sklearn.preprocessing import normalize"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class CFG:\n","    model_path = '/kaggle/input/stable-diffusion-vit-baseline-train/vit_base_patch16_224.pth'\n","    model_name = 'vit_base_patch16_224'\n","    input_size = 224\n","    batch_size = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class DiffusionTestDataset(Dataset):\n","    def __init__(self, images, transform):\n","        self.images = images\n","        self.transform = transform\n","    \n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = Image.open(self.images[idx])\n","        image = self.transform(image)\n","        return image"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def predict(\n","    images,\n","    model_path,\n","    model_name,\n","    input_size,\n","    batch_size\n","):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    transform = transforms.Compose([\n","        transforms.Resize(input_size),\n","        transforms.RandomHorizontalFlip(p=0.5),\n","#         transforms.RandomRotation(degrees=10),\n","\n","        # transforms.RandomVerticalFlip(p=0.5),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","    ])\n","    dataset = DiffusionTestDataset(images, transform)\n","    dataloader = DataLoader(\n","        dataset=dataset,\n","        shuffle=False,\n","        batch_size=batch_size,\n","        pin_memory=True,\n","        num_workers=2,\n","        drop_last=False\n","    )\n","\n","    model = timm.create_model(\n","        model_name,\n","        pretrained=False,\n","        num_classes=384\n","    )\n","    state_dict = torch.load(model_path)\n","    model.load_state_dict(state_dict)\n","    model.to(device)\n","    model.eval()\n","    \n","    tta_preds = None\n","    for _ in range(2):\n","        preds = []\n","        for X in tqdm(dataloader, leave=False):\n","            X = X.to(device)\n","\n","            with torch.no_grad():\n","                X_out = model(X).cpu().numpy()\n","                # L2 normalize -- Start\n","                X_out = X_out / ( np.abs(X_out).max(axis=-1, keepdims=True) + 0.0000001)  # To avoid to overflow at normalize()\n","                X_out = normalize( X_out )\n","                # L2 normalize -- End\n","                preds.append(X_out)\n","                \n","        if tta_preds is None:\n","            tta_preds = np.vstack(preds).flatten()\n","        else:\n","            tta_preds += np.vstack(preds).flatten()\n","    \n","    return tta_preds / 2"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["images = list(Path('/kaggle/input/stable-diffusion-image-to-prompts/images').glob('*.png'))\n","imgIds = [i.stem for i in images]\n","EMBEDDING_LENGTH = 384\n","imgId_eId = [\n","    '_'.join(map(str, i)) for i in zip(\n","        np.repeat(imgIds, EMBEDDING_LENGTH),\n","        np.tile(range(EMBEDDING_LENGTH), len(imgIds)))]\n","\n","embeddings3 = predict(images, CFG.model_path, CFG.model_name, CFG.input_size, CFG.batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["embeddings = embeddings12 + ratio_ViT_B_16 * embeddings3"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission = pd.DataFrame(\n","    index=imgId_eId,\n","    data=embeddings,\n","    columns=['val']\n",").rename_axis('imgId_eId')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission.to_csv('submission.csv')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
